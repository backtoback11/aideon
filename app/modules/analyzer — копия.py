import json
import openai
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

from app.utils import load_api_key
from app.core.file_manager import FileManager


class CodeAnalyzer:
    """
    –ú–æ–¥—É–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞ –∏ (–ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏) –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–∞ —Ä–µ–∂–∏–º–∞:
      1) ChatGPT (GPT-4-turbo) —á–µ—Ä–µ–∑ OpenAI ‚Äî ¬´–º–µ–Ω–µ–¥–∂–µ—Ä/–∞–Ω–∞–ª–∏—Ç–∏–∫¬ª.
      2) StarCoder (–ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å) ‚Äî ¬´–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å –∫–æ–¥–∞¬ª.

    –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã:
      - analyze_code(...) ‚Äî –∞–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ / —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ (—Å —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ–º).
      - generate_code_star_coder(...) ‚Äî –æ—Ç–¥–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ StarCoder‚Äô–æ–º.
    """

    def __init__(self, config=None):
        """
        config ‚Äî —Å–ª–æ–≤–∞—Ä—å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏. –ü—Ä–∏–º–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:
        {
          "model_mode": "ChatGPT" | "StarCoder",  # –∫–∞–∫–æ–π —Ä–µ–∂–∏–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
          "model_name": "gpt-4-turbo",           # –¥–ª—è ChatGPT
          "openai_api_key": "...",
          "local_paths": {
              "StarCoder": "/Users/xxx/models/starcoder/starcoder-7B"
          },
          "use_mps": true,            # –ü—ã—Ç–∞—Ç—å—Å—è –ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ MPS (Mac)
          "temperature": 0.7,
          "max_new_tokens": 512,      # –ú–∞–∫—Å. –∫–æ–ª-–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å)
          "max_context_tokens": 8192  # –ü—Ä–∏–º–µ—Ä–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞
        }
        """
        self.config = config or {}
        self.file_manager = FileManager()

        # --- –æ—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ---
        self.model_mode = self.config.get("model_mode", "ChatGPT")   # "ChatGPT" –∏–ª–∏ "StarCoder"
        self.api_key = load_api_key(self.config)                     # –∫–ª—é—á OpenAI (–º–æ–∂–µ—Ç –±—ã—Ç—å None)
        self.openai_model = self.config.get("model_name", "gpt-4-turbo")

        self.local_paths = self.config.get("local_paths", {})
        self.use_mps = self.config.get("use_mps", True)
        self.temperature = self.config.get("temperature", 0.7)
        self.max_new_tokens = self.config.get("max_new_tokens", 512)
        self.max_context_tokens = self.config.get("max_context_tokens", 8192)

        # --- —Å—Å—ã–ª–∫–∏ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ StarCoder) ---
        self.local_tokenizer = None
        self.local_model = None
        self.current_device = "cpu"

        # –§–ª–∞–≥ –¥–ª—è –æ—Ç–º–µ–Ω—ã —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª–µ–Ω, –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–µ–Ω)
        self.loading_cancelled = False

        # –ï—Å–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–±—Ä–∞–Ω–∞ StarCoder ‚Äî –ø–æ–¥–≥—Ä—É–∂–∞–µ–º —Å—Ä–∞–∑—É
        if self.model_mode == "StarCoder":
            self._load_local_model("StarCoder")
        else:
            print("–ò—Å–ø–æ–ª—å–∑—É–µ–º ChatGPT (OpenAI).")

    # ----------------------------------------------------------------
    # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É ChatGPT / StarCoder
    # ----------------------------------------------------------------
    def switch_model(self, mode):
        """
        –ú–µ–Ω—è–µ–º —Ä–µ–∂–∏–º: "ChatGPT" –∏–ª–∏ "StarCoder".
        –ï—Å–ª–∏ ChatGPT ‚Äî –≤—ã–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –±—ã–ª–∞).
        –ï—Å–ª–∏ StarCoder ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ (–∏–ª–∏ —Ñ–æ–Ω–æ–º).
        """
        if mode not in ["ChatGPT", "StarCoder"]:
            print(f"‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä–µ–∂–∏–º: {mode}")
            return

        self.model_mode = mode
        print(f"üîÅ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ —Ä–µ–∂–∏–º: {mode}")

        if mode == "ChatGPT":
            print("–ò—Å–ø–æ–ª—å–∑—É–µ–º ChatGPT (OpenAI).")
            self.unload_local_model()
        else:
            self._load_local_model("StarCoder")

    def _load_local_model(self, mode):
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (StarCoder).
        –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –ø—Ä–æ–≥—Ä–µ—Å—Å ‚Äî —Å–º. load_model_in_background.
        """
        self.unload_local_model()  # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π

        local_path = self.local_paths.get(mode)
        if not local_path:
            raise ValueError(f"–ù–µ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ '{mode}' –≤ config['local_paths'].")

        print(f"üîΩ –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {mode} –∏–∑ {local_path}...")
        device_map = "mps" if self.use_mps else "cpu"

        try:
            self.local_tokenizer = AutoTokenizer.from_pretrained(
                local_path,
                local_files_only=True,
                trust_remote_code=True
            )
            self.local_model = AutoModelForCausalLM.from_pretrained(
                local_path,
                local_files_only=True,
                trust_remote_code=True,
                torch_dtype=torch.float32,
                device_map=device_map
            )
            self.current_device = device_map
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {mode} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device_map}!")

        except RuntimeError as e:
            err_str = str(e).lower()
            if "out of memory" in err_str and device_map == "mps":
                print("‚ö†Ô∏è MPS OOM. –ü—ã—Ç–∞–µ–º—Å—è fallback –Ω–∞ CPU...")
                try:
                    self.local_model = AutoModelForCausalLM.from_pretrained(
                        local_path,
                        local_files_only=True,
                        trust_remote_code=True,
                        torch_dtype=torch.float32,
                        device_map="cpu"
                    )
                    self.current_device = "cpu"
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å {mode} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU (fallback)!")
                except Exception as e2:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ (CPU fallback): {e2}")
                    self.local_tokenizer = None
                    self.local_model = None
                    self.current_device = "none"
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ {mode}: {e}")
                self.local_tokenizer = None
                self.local_model = None
                self.current_device = "none"
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ {mode}: {e}")
            self.local_tokenizer = None
            self.local_model = None
            self.current_device = "none"

    def unload_local_model(self):
        """–û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å ‚Äî —É–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–æ–¥–µ–ª—å/—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä."""
        if self.local_model is not None:
            print("üóë –í—ã–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏...")
            self.local_model = None
            self.local_tokenizer = None
            self.current_device = "none"
            print("‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≤—ã–≥—Ä—É–∂–µ–Ω–∞.")

    # ----------------------------------------------------------------
    # –§–æ–Ω–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    # ----------------------------------------------------------------
    def load_model_in_background(self, mode, on_progress=None):
        """
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ LoadAIThread.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (success: bool, error_msg: str).
        """
        try:
            self.loading_cancelled = False

            def progress(pct):
                if self.loading_cancelled:
                    raise RuntimeError("Loading canceled by user.")
                if on_progress:
                    on_progress(pct)

            progress(10)
            self.unload_local_model()

            local_path = self.local_paths.get(mode)
            if not local_path:
                return (False, f"–ù–µ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ {mode}")

            progress(20)
            device_map = "mps" if self.use_mps else "cpu"

            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.local_tokenizer = AutoTokenizer.from_pretrained(
                local_path,
                local_files_only=True,
                trust_remote_code=True
            )
            progress(50)

            # –ú–æ–¥–µ–ª—å
            self.local_model = AutoModelForCausalLM.from_pretrained(
                local_path,
                local_files_only=True,
                trust_remote_code=True,
                torch_dtype=torch.float32,
                device_map=device_map
            )
            self.current_device = device_map
            progress(100)

            return (True, None)

        except RuntimeError as e:
            err_str = str(e).lower()
            if "canceled by user" in err_str:
                return (False, "–ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            if "out of memory" in err_str and device_map == "mps":
                # fallback CPU
                try:
                    progress(90)
                    self.local_model = AutoModelForCausalLM.from_pretrained(
                        local_path,
                        local_files_only=True,
                        trust_remote_code=True,
                        torch_dtype=torch.float32,
                        device_map="cpu"
                    )
                    self.current_device = "cpu"
                    if on_progress:
                        on_progress(100)
                    return (True, None)
                except Exception as e2:
                    return (False, f"OOM –ø—Ä–∏ fallback –Ω–∞ CPU: {e2}")
            else:
                return (False, f"Runtime –æ—à–∏–±–∫–∞: {e}")

        except Exception as e:
            return (False, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ {mode}: {e}")

    def cancel_loading(self):
        """–ï—Å–ª–∏ –Ω—É–∂–Ω–æ –æ—Ç–º–µ–Ω–∏—Ç—å —Ñ–æ–Ω–æ–≤—É—é –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏."""
        self.loading_cancelled = True

    # ----------------------------------------------------------------
    # –ú–µ—Ç–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞ (ChatGPT / StarCoder) ‚Äî –ß–ê–ù–ö–û–í–ê–ù–ò–ï
    # ----------------------------------------------------------------
    def analyze_code(self, code_text, file_path=None):
        """
        –ï—Å–ª–∏ ¬´—Ç–µ–∫—Å—Ç –∫–æ–¥–∞¬ª –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π ‚Üí —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏,
        –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º (–ø—Ä–∏–º–µ—Ä–Ω–æ) –∏ —Å–æ–±–∏—Ä–∞–µ–º –æ–±—â–∏–π JSON-–æ—Ç–≤–µ—Ç.
        
        –ü—Ä–∏–º–µ–Ω–∏–º–æ –∫–∞–∫ –¥–ª—è ChatGPT, —Ç–∞–∫ –∏ –¥–ª—è StarCoder, –Ω–æ
        –æ–±—ã—á–Ω–æ ChatGPT ¬´–≤—ã–¥–∞—ë—Ç¬ª –∞–Ω–∞–ª–∏–∑, –∞ StarCoder ¬´–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å¬ª.
        """
        chunks = self._split_into_chunks(code_text, self.max_context_tokens)
        if len(chunks) <= 1:
            return self._analyze_single_chunk(chunks[0], file_path)

        # –°–æ–±–∏—Ä–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π JSON, —Å–∫–ª–µ–∏–≤–∞—è –æ—Ç–≤–µ—Ç—ã –æ—Ç –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
        combined = {
            "chat": "",
            "problems": "",
            "plan": "",
            "process": "",
            "result": "",
            "code": ""
        }
        for i, chunk_text in enumerate(chunks, start=1):
            result_str = self._analyze_single_chunk(
                chunk_text,
                f"{file_path or '–±–µ–∑ –∏–º–µ–Ω–∏'} [chunk {i}/{len(chunks)}]"
            )
            try:
                parsed_json = json.loads(result_str)
                for key in combined.keys():
                    if key in parsed_json and parsed_json[key]:
                        combined[key] += f"\n\n[CHUNK {i}] {parsed_json[key]}"
            except json.JSONDecodeError:
                # –µ—Å–ª–∏ –Ω–µ JSON ‚Äî –ø–∏—à–µ–º –æ—à–∏–±–∫—É –≤ 'chat'
                combined["chat"] += f"\n\n[CHUNK {i} ERROR]\n{result_str}"

        return json.dumps(combined, ensure_ascii=False, indent=2)

    def _analyze_single_chunk(self, code_chunk, file_path=None):
        """
        –°—é–¥–∞ –ø—Ä–∏—Ö–æ–¥–∏—Ç 1 —á–∞–Ω–∫ –∫–æ–¥–∞. –õ–∏–±–æ ChatGPT, –ª–∏–±–æ StarCoder.
        –í–æ–∑–≤—Ä–∞—â–∞–µ–º JSON (string).
        """
        project_tree = self.file_manager.get_project_tree("app")

        context_prompt = (
            "–¢—ã ‚Äî Aideon, AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∫–æ–¥–∞.\n"
            f"–¢–µ–±–µ –¥–∞–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞:\n{project_tree}\n\n"
            "–¢–µ–ø–µ—Ä—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–¥:\n"
            f"{code_chunk}\n\n"
            "–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ —Å –∫–ª—é—á–∞–º–∏:\n"
            "{\n"
            "  \"chat\": \"...\",\n"
            "  \"problems\": \"...\",\n"
            "  \"plan\": \"...\",\n"
            "  \"process\": \"...\",\n"
            "  \"result\": \"...\",\n"
            "  \"code\": \"...\"\n"
            "}\n"
            "–ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ, –∫—Ä–æ–º–µ JSON."
        )

        if self.model_mode == "ChatGPT":
            # –û–±—Ä–∞—â–∞–µ–º—Å—è –∫ OpenAI
            openai.api_key = self.api_key
            try:
                resp = openai.ChatCompletion.create(
                    model=self.openai_model,
                    messages=[
                        {
                            "role": "system",
                            "content": context_prompt
                        },
                        {
                            "role": "user",
                            "content": (
                                f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–¥ –∏–∑ —Ñ–∞–π–ª–∞ {file_path or '–±–µ–∑ –∏–º–µ–Ω–∏'}:\n{code_chunk}"
                            )
                        }
                    ],
                    temperature=self.temperature
                )
                return resp["choices"][0]["message"]["content"]
            except Exception as e:
                return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI: {e}"

        else:
            # StarCoder (–ª–æ–∫–∞–ª—å–Ω–∞—è)
            if not self.local_model or not self.local_tokenizer:
                return "–û—à–∏–±–∫–∞: –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞/–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞."

            full_prompt = (
                f"{context_prompt}\n\n"
                f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–¥ –∏–∑ —Ñ–∞–π–ª–∞ {file_path or '–±–µ–∑ –∏–º–µ–Ω–∏'}:\n{code_chunk}"
            )
            try:
                inputs = self.local_tokenizer(full_prompt, return_tensors="pt")
                device = next(self.local_model.parameters()).device
                inputs = inputs.to(device)

                out_tokens = self.local_model.generate(
                    **inputs,
                    max_new_tokens=self.max_new_tokens
                )
                return self.local_tokenizer.decode(
                    out_tokens[0],
                    skip_special_tokens=True
                )
            except Exception as e:
                return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}"

    # ----------------------------------------------------------------
    # –û—Ç–¥–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –≤ StarCoder
    # ----------------------------------------------------------------
    def generate_code_star_coder(self, prompt_text):
        """
        –ï—Å–ª–∏ —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å StarCoder –∏–º–µ–Ω–Ω–æ –∫–∞–∫ ¬´–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è –∫–æ–¥–∞¬ª:
        - –ü–æ–¥–∞—ë–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é/–∑–∞–≥–æ–ª–æ–≤–æ–∫, 
        - –ü–æ–ª—É—á–∞–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—Å—Ç—Ä–æ–∫—É).
        
        –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –≤—ã –º–æ–∂–µ—Ç–µ –≤—ã–∑—ã–≤–∞—Ç—å, –∫–æ–≥–¥–∞ ChatGPT 
        ¬´–¥–∞–ª –∑–∞–¥–∞–Ω–∏–µ¬ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—É—Å–æ–∫ –∫–æ–¥–∞ (–∏–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–∞–∂–∞–ª ¬´–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å¬ª).
        """
        if self.model_mode != "StarCoder":
            return "–û—à–∏–±–∫–∞: –†–µ–∂–∏–º StarCoder –Ω–µ –∞–∫—Ç–∏–≤–µ–Ω. –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ StarCoder."
        if not self.local_model or not self.local_tokenizer:
            return "–û—à–∏–±–∫–∞: –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å StarCoder –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞."

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π, –±–µ–∑ ¬´JSON only¬ª)
        try:
            inputs = self.local_tokenizer(prompt_text, return_tensors="pt")
            device = next(self.local_model.parameters()).device
            inputs = inputs.to(device)

            gen_tokens = self.local_model.generate(
                **inputs,
                max_new_tokens=self.max_new_tokens,
                temperature=self.temperature,
                do_sample=True   # –Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–∞–∑—Ä–µ—à–∞–µ–º —Å—ç–º–ø–ª–∏–Ω–≥
            )
            return self.local_tokenizer.decode(
                gen_tokens[0],
                skip_special_tokens=True
            )
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ (StarCoder): {e}"

    # ----------------------------------------------------------------
    # –ú–µ—Ç–æ–¥ _split_into_chunks: –æ–±—â–∏–π –¥–ª—è ChatGPT / StarCoder
    # ----------------------------------------------------------------
    def _split_into_chunks(self, text, max_ctx: int):
        """
        –î—Ä–æ–±–∏–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ max_ctx ¬´—Ç–æ–∫–µ–Ω–æ–≤¬ª.
        –î–ª—è ChatGPT –±–µ—Ä—ë–º –∫–æ–ª-–≤–æ —Å–ª–æ–≤ –∫–∞–∫ –ø—Ä–∏–±–ª–∏–∂—ë–Ω–Ω–æ–µ –∫–æ–ª-–≤–æ —Ç–æ–∫–µ–Ω–æ–≤.
        –î–ª—è StarCoder ‚Äî —Ä–µ–∞–ª—å–Ω—ã–π tokenizer.encode().
        """
        text = text.strip()
        if not text:
            return [""]

        if self.model_mode == "ChatGPT":
            # –ü—Ä–∏–º–µ—Ä–Ω–æ —Å—á–∏—Ç–∞–µ–º: 1 —Å–ª–æ–≤–æ ~ 1 —Ç–æ–∫–µ–Ω
            words = text.split()
            if len(words) <= max_ctx:
                return [text]
            chunks = []
            for i in range(0, len(words), max_ctx):
                chunk_slice = words[i:i + max_ctx]
                chunk_text = " ".join(chunk_slice)
                chunks.append(chunk_text)
            return chunks
        else:
            # StarCoder
            if not self.local_tokenizer:
                return [text]
            tokens = self.local_tokenizer.encode(text)
            if len(tokens) <= max_ctx:
                return [text]
            chunks = []
            start_i = 0
            while start_i < len(tokens):
                end_i = start_i + max_ctx
                sub_tokens = tokens[start_i:end_i]
                sub_text = self.local_tokenizer.decode(sub_tokens, skip_special_tokens=True)
                chunks.append(sub_text)
                start_i = end_i
            return chunks

    # ----------------------------------------------------------------
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤ (CPU/RAM)
    # ----------------------------------------------------------------
    def get_resource_stats(self):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –≤–∏–¥–∞:
        {
            "device": "cpu" | "mps" | "none",
            "cpu_percent": float –∏–ª–∏ None,
            "ram_used_mb": int –∏–ª–∏ None
        }
        """
        stats = {
            "device": self.current_device,
            "cpu_percent": None,
            "ram_used_mb": None
        }
        if PSUTIL_AVAILABLE:
            cpu_perc = psutil.cpu_percent(interval=None)
            mem_info = psutil.virtual_memory()
            ram_mb = int(mem_info.used / (1024 * 1024))
            stats["cpu_percent"] = cpu_perc
            stats["ram_used_mb"] = ram_mb
        return stats